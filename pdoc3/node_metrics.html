<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>splink_graph.node_metrics API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>splink_graph.node_metrics</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pyspark
import networkx as nx
import pandas as pd
from pyspark.sql.types import (
    LongType,
    StringType,
    FloatType,
    IntegerType,
    DoubleType,
    StructType,
    StructField,
)
import pyspark.sql.functions as f
from pyspark.sql.functions import pandas_udf, PandasUDFType
from networkx.algorithms.centrality import (
    eigenvector_centrality,
    harmonic_centrality,
)


def eigencentrality(
    sparkdf,
    src=&#34;src&#34;,
    dst=&#34;dst&#34;,
    cluster_id_colname=&#34;cluster_id&#34;,
):

    &#34;&#34;&#34;
        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            distance_colname: distance column name
            cluster_id_colname: Graphframes-created connected components created cluster_id

        Returns:
            node_id:
            eigen_centrality: eigenvector centrality of cluster cluster_id
            cluster_id: cluster_id corresponding to the node_id

    Eigenvector Centrality is an algorithm that measures the transitive influence or connectivity of nodes.
    Eigenvector Centrality was proposed by Phillip Bonacich, in his 1986 paper Power and Centrality:
    A Family of Measures.
    It was the first of the centrality measures that considered the transitive importance of a node in a graph,
    rather than only considering its direct importance.
    Relationships to high-scoring nodes contribute more to the score of a node than connections to low-scoring nodes.
    A high score means that a node is connected to other nodes that have high scores.

    example input spark dataframe


    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0|   0.329|
    |  f|  g|  0.34|         0|   0.659|
    |  b|  c|  0.56|8589934592|   0.439|
    |  g|  h|  0.99|         0|   0.010|
    |  a|  b|   0.4|8589934592|     0.6|
    |  h|  i|   0.5|         0|     0.5|
    |  h|  j|   0.8|         0|   0.199|
    |  d|  e|  0.84|         0|   0.160|
    |  e|  f|  0.65|         0|    0.35|


    example output spark dataframe


    |node_id|   eigen_centrality|cluster_id|
    |-------|-------------------|----------|
    |   b   |  0.707106690085642|8589934592|
    |   c   | 0.5000000644180599|8589934592|
    |   a   | 0.5000000644180599|8589934592|
    |   f   | 0.5746147732828122|         0|
    |   d   | 0.4584903903420785|         0|
    |   g   |0.37778352393858183|         0|
    |   h   |0.27663243805676946|         0|
    |   i   |0.12277029263709134|         0|
    |   j   |0.12277029263709134|         0|
    |   e   | 0.4584903903420785|         0|



    &#34;&#34;&#34;
    ecschema = StructType(
        [
            StructField(&#34;node_id&#34;, StringType()),
            StructField(&#34;eigen_centrality&#34;, DoubleType()),
            StructField(cluster_id_colname, LongType()),
        ]
    )

    psrc = src
    pdst = dst

    @pandas_udf(ecschema, PandasUDFType.GROUPED_MAP)
    def eigenc(pdf: pd.DataFrame) -&gt; pd.DataFrame:
        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst)
        ec = eigenvector_centrality(nxGraph, tol=1e-03)
        out_df = (
            pd.DataFrame.from_dict(ec, orient=&#34;index&#34;, columns=[&#34;eigen_centrality&#34;])
            .reset_index()
            .rename(
                columns={&#34;index&#34;: &#34;node_id&#34;, &#34;eigen_centrality&#34;: &#34;eigen_centrality&#34;}
            )
        )

        cluster_id = pdf[cluster_id_colname][0]
        out_df[cluster_id_colname] = cluster_id
        return out_df

    out = sparkdf.groupby(cluster_id_colname).apply(eigenc)
    return out


def harmoniccentrality(sparkdf, src=&#34;src&#34;, dst=&#34;dst&#34;, cluster_id_colname=&#34;cluster_id&#34;):

    &#34;&#34;&#34;
        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            distance_colname: distance column name
            cluster_id_colname: Graphframes-created connected components created cluster_id

        Returns:
            node_id:
            harmonic_centrality: Harmonic centrality of cluster cluster_id
            cluster_id: cluster_id corresponding to the node_id

    Harmonic centrality (also known as valued centrality) is a variant of closeness centrality, that was invented
    to solve the problem the original formula had when dealing with unconnected graphs.
    Harmonic centrality was proposed by Marchiori and Latora  while trying to come up with a sensible notion of &#34;average shortest path&#34;.
    They suggested a different way of calculating the average distance to that used in the Closeness Centrality algorithm.
    Rather than summing the distances of a node to all other nodes, the harmonic centrality algorithm sums the inverse of those distances.
    This enables it deal with infinite values.


    input spark dataframe:

    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0|   0.329|
    |  f|  g|  0.34|         0|   0.659|
    |  b|  c|  0.56|8589934592|   0.439|
    |  g|  h|  0.99|         0|   0.010|
    |  a|  b|   0.4|8589934592|     0.6|
    |  h|  i|   0.5|         0|     0.5|
    |  h|  j|   0.8|         0|   0.199|
    |  d|  e|  0.84|         0|   0.160|
    |  e|  f|  0.65|         0|    0.35|

    output spark dataframe:


    |node_id|harmonic_centrality|cluster_id|
    |-------|-------------------|----------|
    |   b   |                2.0|8589934592|
    |   c   |                1.5|8589934592|
    |   a   |                1.5|8589934592|
    |   f   |  4.166666666666667|         0|
    |   d   | 3.3333333333333335|         0|
    |   g   |                4.0|         0|
    |   h   |  4.166666666666667|         0|
    |   i   | 2.8333333333333335|         0|
    |   j   | 2.8333333333333335|         0|
    |   e   | 3.3333333333333335|         0|

    &#34;&#34;&#34;

    hcschema = StructType(
        [
            StructField(&#34;node_id&#34;, StringType()),
            StructField(&#34;harmonic_centrality&#34;, DoubleType()),
            StructField(cluster_id_colname, LongType()),
        ]
    )

    psrc = src
    pdst = dst

    @pandas_udf(hcschema, PandasUDFType.GROUPED_MAP)
    def harmc(pdf: pd.DataFrame) -&gt; pd.DataFrame:
        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst)
        hc = harmonic_centrality(nxGraph)
        out_df = (
            pd.DataFrame.from_dict(hc, orient=&#34;index&#34;, columns=[&#34;harmonic_centrality&#34;])
            .reset_index()
            .rename(
                columns={
                    &#34;index&#34;: &#34;node_id&#34;,
                    &#34;harmonic_centrality&#34;: &#34;harmonic_centrality&#34;,
                }
            )
        )
        cluster_id = pdf[cluster_id_colname][0]
        out_df[cluster_id_colname] = cluster_id
        return out_df

    out = sparkdf.groupby(cluster_id_colname).apply(harmc)
    return out</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="splink_graph.node_metrics.eigencentrality"><code class="name flex">
<span>def <span class="ident">eigencentrality</span></span>(<span>sparkdf, src='src', dst='dst', cluster_id_colname='cluster_id')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<pre><code>sparkdf: imput edgelist Spark DataFrame
src: src column name
dst: dst column name
distance_colname: distance column name
cluster_id_colname: Graphframes-created connected components created cluster_id
</code></pre>
<p>Returns:
node_id:
eigen_centrality: eigenvector centrality of cluster cluster_id
cluster_id: cluster_id corresponding to the node_id
Eigenvector Centrality is an algorithm that measures the transitive influence or connectivity of nodes.
Eigenvector Centrality was proposed by Phillip Bonacich, in his 1986 paper Power and Centrality:
A Family of Measures.
It was the first of the centrality measures that considered the transitive importance of a node in a graph,
rather than only considering its direct importance.
Relationships to high-scoring nodes contribute more to the score of a node than connections to low-scoring nodes.
A high score means that a node is connected to other nodes that have high scores.</p>
<p>example input spark dataframe</p>
<table>
<thead>
<tr>
<th>src</th>
<th>dst</th>
<th>weight</th>
<th>cluster_id</th>
<th>distance</th>
</tr>
</thead>
<tbody>
<tr>
<td>f</td>
<td>d</td>
<td>0.67</td>
<td>0</td>
<td>0.329</td>
</tr>
<tr>
<td>f</td>
<td>g</td>
<td>0.34</td>
<td>0</td>
<td>0.659</td>
</tr>
<tr>
<td>b</td>
<td>c</td>
<td>0.56</td>
<td>8589934592</td>
<td>0.439</td>
</tr>
<tr>
<td>g</td>
<td>h</td>
<td>0.99</td>
<td>0</td>
<td>0.010</td>
</tr>
<tr>
<td>a</td>
<td>b</td>
<td>0.4</td>
<td>8589934592</td>
<td>0.6</td>
</tr>
<tr>
<td>h</td>
<td>i</td>
<td>0.5</td>
<td>0</td>
<td>0.5</td>
</tr>
<tr>
<td>h</td>
<td>j</td>
<td>0.8</td>
<td>0</td>
<td>0.199</td>
</tr>
<tr>
<td>d</td>
<td>e</td>
<td>0.84</td>
<td>0</td>
<td>0.160</td>
</tr>
<tr>
<td>e</td>
<td>f</td>
<td>0.65</td>
<td>0</td>
<td>0.35</td>
</tr>
</tbody>
</table>
<p>example output spark dataframe</p>
<table>
<thead>
<tr>
<th>node_id</th>
<th>eigen_centrality</th>
<th>cluster_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>b</td>
<td>0.707106690085642</td>
<td>8589934592</td>
</tr>
<tr>
<td>c</td>
<td>0.5000000644180599</td>
<td>8589934592</td>
</tr>
<tr>
<td>a</td>
<td>0.5000000644180599</td>
<td>8589934592</td>
</tr>
<tr>
<td>f</td>
<td>0.5746147732828122</td>
<td>0</td>
</tr>
<tr>
<td>d</td>
<td>0.4584903903420785</td>
<td>0</td>
</tr>
<tr>
<td>g</td>
<td>0.37778352393858183</td>
<td>0</td>
</tr>
<tr>
<td>h</td>
<td>0.27663243805676946</td>
<td>0</td>
</tr>
<tr>
<td>i</td>
<td>0.12277029263709134</td>
<td>0</td>
</tr>
<tr>
<td>j</td>
<td>0.12277029263709134</td>
<td>0</td>
</tr>
<tr>
<td>e</td>
<td>0.4584903903420785</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def eigencentrality(
    sparkdf,
    src=&#34;src&#34;,
    dst=&#34;dst&#34;,
    cluster_id_colname=&#34;cluster_id&#34;,
):

    &#34;&#34;&#34;
        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            distance_colname: distance column name
            cluster_id_colname: Graphframes-created connected components created cluster_id

        Returns:
            node_id:
            eigen_centrality: eigenvector centrality of cluster cluster_id
            cluster_id: cluster_id corresponding to the node_id

    Eigenvector Centrality is an algorithm that measures the transitive influence or connectivity of nodes.
    Eigenvector Centrality was proposed by Phillip Bonacich, in his 1986 paper Power and Centrality:
    A Family of Measures.
    It was the first of the centrality measures that considered the transitive importance of a node in a graph,
    rather than only considering its direct importance.
    Relationships to high-scoring nodes contribute more to the score of a node than connections to low-scoring nodes.
    A high score means that a node is connected to other nodes that have high scores.

    example input spark dataframe


    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0|   0.329|
    |  f|  g|  0.34|         0|   0.659|
    |  b|  c|  0.56|8589934592|   0.439|
    |  g|  h|  0.99|         0|   0.010|
    |  a|  b|   0.4|8589934592|     0.6|
    |  h|  i|   0.5|         0|     0.5|
    |  h|  j|   0.8|         0|   0.199|
    |  d|  e|  0.84|         0|   0.160|
    |  e|  f|  0.65|         0|    0.35|


    example output spark dataframe


    |node_id|   eigen_centrality|cluster_id|
    |-------|-------------------|----------|
    |   b   |  0.707106690085642|8589934592|
    |   c   | 0.5000000644180599|8589934592|
    |   a   | 0.5000000644180599|8589934592|
    |   f   | 0.5746147732828122|         0|
    |   d   | 0.4584903903420785|         0|
    |   g   |0.37778352393858183|         0|
    |   h   |0.27663243805676946|         0|
    |   i   |0.12277029263709134|         0|
    |   j   |0.12277029263709134|         0|
    |   e   | 0.4584903903420785|         0|



    &#34;&#34;&#34;
    ecschema = StructType(
        [
            StructField(&#34;node_id&#34;, StringType()),
            StructField(&#34;eigen_centrality&#34;, DoubleType()),
            StructField(cluster_id_colname, LongType()),
        ]
    )

    psrc = src
    pdst = dst

    @pandas_udf(ecschema, PandasUDFType.GROUPED_MAP)
    def eigenc(pdf: pd.DataFrame) -&gt; pd.DataFrame:
        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst)
        ec = eigenvector_centrality(nxGraph, tol=1e-03)
        out_df = (
            pd.DataFrame.from_dict(ec, orient=&#34;index&#34;, columns=[&#34;eigen_centrality&#34;])
            .reset_index()
            .rename(
                columns={&#34;index&#34;: &#34;node_id&#34;, &#34;eigen_centrality&#34;: &#34;eigen_centrality&#34;}
            )
        )

        cluster_id = pdf[cluster_id_colname][0]
        out_df[cluster_id_colname] = cluster_id
        return out_df

    out = sparkdf.groupby(cluster_id_colname).apply(eigenc)
    return out</code></pre>
</details>
</dd>
<dt id="splink_graph.node_metrics.harmoniccentrality"><code class="name flex">
<span>def <span class="ident">harmoniccentrality</span></span>(<span>sparkdf, src='src', dst='dst', cluster_id_colname='cluster_id')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<pre><code>sparkdf: imput edgelist Spark DataFrame
src: src column name
dst: dst column name
distance_colname: distance column name
cluster_id_colname: Graphframes-created connected components created cluster_id
</code></pre>
<p>Returns:
node_id:
harmonic_centrality: Harmonic centrality of cluster cluster_id
cluster_id: cluster_id corresponding to the node_id
Harmonic centrality (also known as valued centrality) is a variant of closeness centrality, that was invented
to solve the problem the original formula had when dealing with unconnected graphs.
Harmonic centrality was proposed by Marchiori and Latora
while trying to come up with a sensible notion of "average shortest path".
They suggested a different way of calculating the average distance to that used in the Closeness Centrality algorithm.
Rather than summing the distances of a node to all other nodes, the harmonic centrality algorithm sums the inverse of those distances.
This enables it deal with infinite values.</p>
<p>input spark dataframe:</p>
<table>
<thead>
<tr>
<th>src</th>
<th>dst</th>
<th>weight</th>
<th>cluster_id</th>
<th>distance</th>
</tr>
</thead>
<tbody>
<tr>
<td>f</td>
<td>d</td>
<td>0.67</td>
<td>0</td>
<td>0.329</td>
</tr>
<tr>
<td>f</td>
<td>g</td>
<td>0.34</td>
<td>0</td>
<td>0.659</td>
</tr>
<tr>
<td>b</td>
<td>c</td>
<td>0.56</td>
<td>8589934592</td>
<td>0.439</td>
</tr>
<tr>
<td>g</td>
<td>h</td>
<td>0.99</td>
<td>0</td>
<td>0.010</td>
</tr>
<tr>
<td>a</td>
<td>b</td>
<td>0.4</td>
<td>8589934592</td>
<td>0.6</td>
</tr>
<tr>
<td>h</td>
<td>i</td>
<td>0.5</td>
<td>0</td>
<td>0.5</td>
</tr>
<tr>
<td>h</td>
<td>j</td>
<td>0.8</td>
<td>0</td>
<td>0.199</td>
</tr>
<tr>
<td>d</td>
<td>e</td>
<td>0.84</td>
<td>0</td>
<td>0.160</td>
</tr>
<tr>
<td>e</td>
<td>f</td>
<td>0.65</td>
<td>0</td>
<td>0.35</td>
</tr>
</tbody>
</table>
<p>output spark dataframe:</p>
<table>
<thead>
<tr>
<th>node_id</th>
<th>harmonic_centrality</th>
<th>cluster_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>b</td>
<td>2.0</td>
<td>8589934592</td>
</tr>
<tr>
<td>c</td>
<td>1.5</td>
<td>8589934592</td>
</tr>
<tr>
<td>a</td>
<td>1.5</td>
<td>8589934592</td>
</tr>
<tr>
<td>f</td>
<td>4.166666666666667</td>
<td>0</td>
</tr>
<tr>
<td>d</td>
<td>3.3333333333333335</td>
<td>0</td>
</tr>
<tr>
<td>g</td>
<td>4.0</td>
<td>0</td>
</tr>
<tr>
<td>h</td>
<td>4.166666666666667</td>
<td>0</td>
</tr>
<tr>
<td>i</td>
<td>2.8333333333333335</td>
<td>0</td>
</tr>
<tr>
<td>j</td>
<td>2.8333333333333335</td>
<td>0</td>
</tr>
<tr>
<td>e</td>
<td>3.3333333333333335</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def harmoniccentrality(sparkdf, src=&#34;src&#34;, dst=&#34;dst&#34;, cluster_id_colname=&#34;cluster_id&#34;):

    &#34;&#34;&#34;
        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            distance_colname: distance column name
            cluster_id_colname: Graphframes-created connected components created cluster_id

        Returns:
            node_id:
            harmonic_centrality: Harmonic centrality of cluster cluster_id
            cluster_id: cluster_id corresponding to the node_id

    Harmonic centrality (also known as valued centrality) is a variant of closeness centrality, that was invented
    to solve the problem the original formula had when dealing with unconnected graphs.
    Harmonic centrality was proposed by Marchiori and Latora  while trying to come up with a sensible notion of &#34;average shortest path&#34;.
    They suggested a different way of calculating the average distance to that used in the Closeness Centrality algorithm.
    Rather than summing the distances of a node to all other nodes, the harmonic centrality algorithm sums the inverse of those distances.
    This enables it deal with infinite values.


    input spark dataframe:

    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0|   0.329|
    |  f|  g|  0.34|         0|   0.659|
    |  b|  c|  0.56|8589934592|   0.439|
    |  g|  h|  0.99|         0|   0.010|
    |  a|  b|   0.4|8589934592|     0.6|
    |  h|  i|   0.5|         0|     0.5|
    |  h|  j|   0.8|         0|   0.199|
    |  d|  e|  0.84|         0|   0.160|
    |  e|  f|  0.65|         0|    0.35|

    output spark dataframe:


    |node_id|harmonic_centrality|cluster_id|
    |-------|-------------------|----------|
    |   b   |                2.0|8589934592|
    |   c   |                1.5|8589934592|
    |   a   |                1.5|8589934592|
    |   f   |  4.166666666666667|         0|
    |   d   | 3.3333333333333335|         0|
    |   g   |                4.0|         0|
    |   h   |  4.166666666666667|         0|
    |   i   | 2.8333333333333335|         0|
    |   j   | 2.8333333333333335|         0|
    |   e   | 3.3333333333333335|         0|

    &#34;&#34;&#34;

    hcschema = StructType(
        [
            StructField(&#34;node_id&#34;, StringType()),
            StructField(&#34;harmonic_centrality&#34;, DoubleType()),
            StructField(cluster_id_colname, LongType()),
        ]
    )

    psrc = src
    pdst = dst

    @pandas_udf(hcschema, PandasUDFType.GROUPED_MAP)
    def harmc(pdf: pd.DataFrame) -&gt; pd.DataFrame:
        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst)
        hc = harmonic_centrality(nxGraph)
        out_df = (
            pd.DataFrame.from_dict(hc, orient=&#34;index&#34;, columns=[&#34;harmonic_centrality&#34;])
            .reset_index()
            .rename(
                columns={
                    &#34;index&#34;: &#34;node_id&#34;,
                    &#34;harmonic_centrality&#34;: &#34;harmonic_centrality&#34;,
                }
            )
        )
        cluster_id = pdf[cluster_id_colname][0]
        out_df[cluster_id_colname] = cluster_id
        return out_df

    out = sparkdf.groupby(cluster_id_colname).apply(harmc)
    return out</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="splink_graph" href="index.html">splink_graph</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="splink_graph.node_metrics.eigencentrality" href="#splink_graph.node_metrics.eigencentrality">eigencentrality</a></code></li>
<li><code><a title="splink_graph.node_metrics.harmoniccentrality" href="#splink_graph.node_metrics.harmoniccentrality">harmoniccentrality</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>