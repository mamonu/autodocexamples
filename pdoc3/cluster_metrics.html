<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>splink_graph.cluster_metrics API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>splink_graph.cluster_metrics</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from pyspark.sql.types import (
    LongType,
    StringType,
    FloatType,
    IntegerType,
    StructType,
    StructField,
)
import pyspark.sql.functions as f
from pyspark.sql.functions import pandas_udf, PandasUDFType
import networkx as nx
import networkx.algorithms.community as nx_comm
from networkx.algorithms.distance_measures import diameter
from networkx.algorithms.cluster import transitivity
from networkx.algorithms.centrality import edge_betweenness_centrality
from networkx.algorithms.bridges import bridges
from networkx.algorithms.community.centrality import girvan_newman
from networkx.algorithms.components import articulation_points
import pandas as pd
import math
from splink_graph.utils import _laplacian_spectrum

# Read on how to setup spark to work around with pandas udf:
# see answers on
# https://stackoverflow.com/questions/58458415/pandas-scalar-udf-failing-illegalargumentexception


def cluster_basic_stats(
    df, src=&#34;src&#34;, dst=&#34;dst&#34;, cluster_id_colname=&#34;cluster_id&#34;, weight_colname=&#34;weight&#34;
):

    &#34;&#34;&#34;show nodecount , edgecount, density and enumerate nodes per cluster/conn. components subgraph

    example input spark dataframe:


    |src|dst|cluster_id|
    |---|---|----------|
    |  f|  d|         0|
    |  f|  g|         0|
    |  b|  c|8589934592|
    |  g|  h|         0|
    |  a|  b|8589934592|
    |  h|  i|         0|
    |  h|  j|         0|
    |  d|  e|         0|
    |  e|  f|         0|


    example output spark dataframe:

    |cluster_id|               nodes|nodecount|edgecount|density|
    |----------|--------------------|---------|---------|------|
    |8589934592|           [b, a, c]|        3|        2|0.666|
    |         0|[h, g, f, e, d, i..]|        7|        7|0.333|


    &#34;&#34;&#34;

    edgec = df.groupby(cluster_id_colname).agg(
        f.count(weight_colname).alias(&#34;edgecount&#34;)
    )
    srcdf = df.groupby(cluster_id_colname).agg(f.collect_set(src).alias(&#34;sources&#34;))
    dstdf = df.groupby(cluster_id_colname).agg(f.collect_set(dst).alias(&#34;destinations&#34;))
    allnodes = srcdf.join(dstdf, on=cluster_id_colname)
    allnodes = allnodes.withColumn(
        &#34;nodes&#34;, f.array_union(f.col(&#34;sources&#34;), f.col(&#34;destinations&#34;))
    ).withColumn(&#34;nodecount&#34;, f.size(f.col(&#34;nodes&#34;)))

    output = allnodes.join(edgec, on=cluster_id_colname)

    # density related calcs based on nodecount and max possible number of edges in an undirected graph

    output = output.withColumn(
        &#34;maxNumberOfEdgesundir&#34;, f.expr(&#34;(nodecount*(nodecount-1))/2&#34;)
    )
    output = output.withColumn(
        &#34;density&#34;, f.expr(&#34;edgecount/maxNumberOfEdgesundir&#34;)
    ).drop(&#34;sources&#34;, &#34;destinations&#34;, &#34;maxNumberOfEdgesundir&#34;)
    output = output.withColumnRenamed(cluster_id_colname, &#34;cluster_id&#34;)

    return output


def cluster_graph_hash(sparkdf, src=&#34;src&#34;, dst=&#34;dst&#34;, cluster_id_colname=&#34;cluster_id&#34;):
    &#34;&#34;&#34;calculate weisfeiler-lehman graph hash of a cluster


    Args:
        sparkdf: imput edgelist Spark DataFrame
        src: src column name
        dst: dst column name
        cluster_id_colname: Graphframes-created connected components created cluster_id
    &#34;&#34;&#34;
    psrc = src
    pdst = dst

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;graphhash&#34;, StringType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def gh(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst)
        h = nx.weisfeiler_lehman_graph_hash(nxGraph)
        co = pdf[cluster_id_colname].iloc[0]  # access component id

        return pd.DataFrame(
            [[co] + [h]],
            columns=[
                &#34;cluster_id&#34;,
                &#34;graphhash&#34;,
            ],
        )

    out = sparkdf.groupby(cluster_id_colname).apply(gh)
    return out


def cluster_graph_hash_edge_attr(
    sparkdf, src=&#34;src&#34;, dst=&#34;dst&#34;, cluster_id_colname=&#34;cluster_id&#34;, edge_attr_col=None
):
    &#34;&#34;&#34;calculate weisfeiler-lehman graph hash of a cluster taking into account the edge weights too.
      weights are converted to strings for the hashing.


    Args:
        sparkdf: imput edgelist Spark DataFrame
        src: src column name
        dst: dst column name
        cluster_id_colname: Graphframes-created connected components created cluster_id
        edge_attr_col: edge attributes (like edge weights) column
    &#34;&#34;&#34;
    psrc = src
    pdst = dst

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;graphhash_ea&#34;, StringType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def gh_edge_attr(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        if edge_attr_col:
            pdf[edge_attr_col] = pdf[edge_attr_col].astype(str)

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst, [edge_attr_col])

        ghe = nx.weisfeiler_lehman_graph_hash(nxGraph, edge_attr=edge_attr_col)
        co = pdf[cluster_id_colname].iloc[0]  # access component id

        return pd.DataFrame(
            [[co] + [ghe]],
            columns=[
                &#34;cluster_id&#34;,
                &#34;graphhash_ea&#34;,
            ],
        )

    out = sparkdf.groupby(cluster_id_colname).apply(gh_edge_attr)
    return out


def cluster_main_stats(sparkdf, src=&#34;src&#34;, dst=&#34;dst&#34;, cluster_id_colname=&#34;cluster_id&#34;):
    &#34;&#34;&#34;calculate diameter / transitivity(GCC) / triangle clustering coefficient LCC / square clustering coeff

        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            cluster_id_colname: Graphframes-created connected components created cluster_id



        input spark dataframe:


    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0| 0.329|
    |  f|  g|  0.34|         0| 0.659|
    |  b|  c|  0.56|8589934592| 0.439|
    |  g|  h|  0.99|         0|0.010|
    |  a|  b|   0.4|8589934592|0.6|
    |  h|  i|   0.5|         0|0.5|
    |  h|  j|   0.8|         0| 0.199|
    |  d|  e|  0.84|         0| 0.160|
    |  e|  f|  0.65|         0|0.35|



        output spark dataframe:




    &#34;&#34;&#34;

    psrc = src
    pdst = dst

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;diameter&#34;, IntegerType()),
                StructField(&#34;transitivity&#34;, FloatType()),
                StructField(&#34;tri_clustcoeff&#34;, FloatType()),
                StructField(&#34;sq_clustcoeff&#34;, FloatType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def drt(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst)
        d = diameter(nxGraph)
        t = transitivity(nxGraph)
        tric = nx.average_clustering(nxGraph)
        sq = nx.square_clustering(nxGraph)
        sqc = sum(sq.values()) / len(sq.values())

        co = pdf[cluster_id_colname].iloc[0]  # access component id

        return pd.DataFrame(
            [[co] + [d] + [t] + [tric] + [sqc]],
            columns=[
                &#34;cluster_id&#34;,
                &#34;diameter&#34;,
                &#34;transitivity&#34;,
                &#34;tri_clustcoeff&#34;,
                &#34;sq_clustcoeff&#34;,
            ],
        )

    out = sparkdf.groupby(cluster_id_colname).apply(drt)

    out = (
        out.withColumn(&#34;tri_clustcoeff&#34;, f.round(f.col(&#34;tri_clustcoeff&#34;), 3))
        .withColumn(&#34;sq_clustcoeff&#34;, f.round(f.col(&#34;sq_clustcoeff&#34;), 3))
        .withColumn(&#34;transitivity&#34;, f.round(f.col(&#34;transitivity&#34;), 3))
    )
    return out


def cluster_connectivity_stats(
    sparkdf,
    src=&#34;src&#34;,
    dst=&#34;dst&#34;,
    cluster_id_colname=&#34;cluster_id&#34;,
):
    &#34;&#34;&#34;outputs connectivity metrics per cluster_id

        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            cluster_id_colname: Graphframes-created connected components created cluster_id

        Returns:

            node_conn: Node Connectivity measures the minimal number of vertices that can be removed to disconnect the graph.
            edge_conn: Edge connectivity measures the minimal number of edges that can be removed to disconnect the graph.
            degeneracy: a way to measure sparsity
            num_articulation_pts: how many articulation points? an articulation point is a node that if removed disconnects a graph


        The larger these metrics are --&gt; the more connected the subggraph is.



        input spark dataframe:


    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0| 0.329|
    |  f|  g|  0.34|         0| 0.659|
    |  b|  c|  0.56|8589934592| 0.439|
    |  g|  h|  0.99|         0|0.010|
    |  a|  b|   0.4|8589934592|0.6|
    |  h|  i|   0.5|         0|0.5|
    |  h|  j|   0.8|         0| 0.199|
    |  d|  e|  0.84|         0| 0.160|
    |  e|  f|  0.65|         0|0.35|



        output spark dataframe:

    &#34;&#34;&#34;

    psrc = src
    pdst = dst

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;node_conn&#34;, IntegerType()),
                StructField(&#34;edge_conn&#34;, IntegerType()),
                StructField(&#34;degeneracy&#34;, IntegerType()),
                StructField(&#34;num_articulation_pts&#34;, IntegerType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def conn_eff(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst)
        nc = nx.algorithms.node_connectivity(nxGraph)
        ec = nx.algorithms.edge_connectivity(nxGraph)
        dg = max(nx.core_number(nxGraph).values())

        ap = articulation_points(nxGraph)
        num_aps = len(list(ap))

        co = pdf[cluster_id_colname].iloc[0]  # access component id

        return pd.DataFrame(
            [[co] + [nc] + [ec] + [dg] + [num_aps]],
            columns=[
                &#34;cluster_id&#34;,
                &#34;node_conn&#34;,
                &#34;edge_conn&#34;,
                &#34;degeneracy&#34;,
                &#34;num_articulation_pts&#34;,
            ],
        )

    out = sparkdf.groupby(cluster_id_colname).apply(conn_eff)

    return out


def cluster_eb_modularity(
    sparkdf,
    src=&#34;src&#34;,
    dst=&#34;dst&#34;,
    distance_colname=&#34;distance&#34;,
    cluster_id_colname=&#34;cluster_id&#34;,
):
    &#34;&#34;&#34;
        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            distance_colname: column name where edge distance (1-weight) is available
            cluster_id_colname: column that contains Graphframes-created connected components created cluster_id

        Returns:
            cluster_id: connected components created cluster_id
            comp_eb_modularity: modularity for cluster_id if it partitioned into 2 parts at the point where the highest edge betweenness exists


        example input spark dataframe


    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0| 0.329|
    |  f|  g|  0.34|         0| 0.659|
    |  b|  c|  0.56|8589934592| 0.439|
    |  g|  h|  0.99|         0|0.010|
    |  a|  b|   0.4|8589934592|0.6|
    |  h|  i|   0.5|         0|0.5|
    |  h|  j|   0.8|         0| 0.199|
    |  d|  e|  0.84|         0| 0.160|
    |  e|  f|  0.65|         0|0.35|


        example output spark dataframe

    |cluster_id|comp_eb_modularity|
    |----------|----------------|
    |         0| 0.400|
    |8589934592| -0.04|

    &#34;&#34;&#34;

    psrc = src
    pdst = dst
    pdistance = distance_colname

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;cluster_eb_modularity&#34;, FloatType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def cluster_eb_m(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst, pdistance)

        ## TODO: document this code
        # this is a method that calculates the modularity of a cluster if partitioned into 2 parts
        # where the split is happening where the highest edge betweeness is.

        # if modularity is negative :
        #      that means that the split just leaves singleton nodes or something like that.
        #      basically the cluster is of no interest
        # if modularity is 0 or very close to 0 :
        #      its a cluster of well connected nodes so... nothing to see here really.
        # if modularity is around 0.3+ then :
        #      its a cluster of possible interest

        def largest_edge_betweenness(G):
            centrality = edge_betweenness_centrality(
                G, weight=pdistance, normalized=True
            )
            return max(centrality, key=centrality.get)

        comp = girvan_newman(nxGraph, most_valuable_edge=largest_edge_betweenness)
        gn = tuple(sorted(c) for c in next(comp))

        co = pdf[cluster_id_colname].iloc[0]  # access component id
        nc = nx.number_of_nodes(nxGraph)

        if nc &gt; 2:
            try:
                co_eb_mod = nx_comm.modularity(nxGraph, gn)
            except ZeroDivisionError:
                raise Exception(
                    f&#34;ZeroDivisionError on component id {co}. &#34;
                    &#34;This can occur if one of the weights (distances) is zero.&#34;
                )
        else:
            co_eb_mod = -1.0

        return pd.DataFrame(
            [[co] + [co_eb_mod]],
            columns=[
                &#34;cluster_id&#34;,
                &#34;cluster_eb_modularity&#34;,
            ],
        )

    out = sparkdf.groupby(cluster_id_colname).apply(cluster_eb_m)

    return out


def cluster_lpg_modularity(
    sparkdf,
    src=&#34;src&#34;,
    dst=&#34;dst&#34;,
    distance_colname=&#34;distance&#34;,
    cluster_id_colname=&#34;cluster_id&#34;,
):
    &#34;&#34;&#34;
        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            distance_colname: column name where edge distance (1-weight) is available
            cluster_id_colname: column that contains Graphframes-created connected components created cluster_id

        Returns:
            cluster_id: connected components created cluster_id
            cluster_lpg_modularity: modularity for cluster_id if it partitioned into 2 parts based on label propagation


        example input spark dataframe


    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0| 0.329|
    |  f|  g|  0.34|         0| 0.659|
    |  b|  c|  0.56|8589934592| 0.439|
    |  g|  h|  0.99|         0|0.010|
    |  a|  b|   0.4|8589934592|0.6|
    |  h|  i|   0.5|         0|0.5|
    |  h|  j|   0.8|         0| 0.199|
    |  d|  e|  0.84|         0| 0.160|
    |  e|  f|  0.65|         0|0.35|


        example output spark dataframe

    |cluster_id|cluster_lpg_modularity|
    |----------|----------------|
    |         0| 0.400|
    |8589934592| -0.04|

    &#34;&#34;&#34;

    psrc = src
    pdst = dst
    pdistance = distance_colname

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;cluster_lpg_modularity&#34;, FloatType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def cluster_lpg_m(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst, pdistance)

        ## TODO: document this code
        # this is a method that calculates the modularity of a cluster if partitioned into 2 parts
        # where the split is happening based on label propagation clustering.

        # if modularity is negative :
        #      that means that the split just leaves singleton nodes or something like that.
        #      basically the cluster is of no interest
        # if modularity is 0 or very close to 0 :
        #      its a cluster of well connected nodes so... nothing to see here really.
        # if modularity is around 0.3+ then :
        #      its a cluster of possible interest

        gn = list(nx_comm.label_propagation_communities(nxGraph))

        co = pdf[cluster_id_colname].iloc[0]  # access component id
        co_lpg_mod = nx_comm.modularity(nxGraph, gn)

        return pd.DataFrame(
            [[co] + [co_lpg_mod]],
            columns=[
                &#34;cluster_id&#34;,
                &#34;cluster_lpg_modularity&#34;,
            ],
        )

    out = sparkdf.groupby(cluster_id_colname).apply(cluster_lpg_m)

    return out


def cluster_avg_edge_betweenness(
    sparkdf,
    src=&#34;src&#34;,
    dst=&#34;dst&#34;,
    distance_colname=&#34;distance&#34;,
    cluster_id_colname=&#34;cluster_id&#34;,
):
    &#34;&#34;&#34;
        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            distance_colname: column name where edge distance (1-weight) is available
            cluster_id_colname: column that contains Graphframes-created connected components created cluster_id

        Returns:
            cluster_id: connected components created cluster_id
            avg_cluster_eb: average edge betweeness for cluster_id

        input spark dataframe:


    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0| 0.329|
    |  f|  g|  0.34|         0| 0.659|
    |  b|  c|  0.56|8589934592| 0.439|
    |  g|  h|  0.99|         0|0.010|
    |  a|  b|   0.4|8589934592|0.6|
    |  h|  i|   0.5|         0|0.5|
    |  h|  j|   0.8|         0| 0.199|
    |  d|  e|  0.84|         0| 0.160|
    |  e|  f|  0.65|         0|0.35|



        output spark dataframe:

    &#34;&#34;&#34;

    psrc = src
    pdst = dst
    pdistance = distance_colname

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;avg_cluster_eb&#34;, FloatType()),
                StructField(&#34;wiener_ind&#34;, FloatType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def avg_eb(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst, pdistance)
        w = nx.wiener_index(nxGraph)

        edge_btwn = edge_betweenness_centrality(nxGraph, normalized=True)

        if len(edge_btwn) &gt; 0:
            aeb = round(sum(list(edge_btwn.values())) / len(edge_btwn), 3)
        else:
            aeb = 0.0

        co = pdf[cluster_id_colname].iloc[0]  # access component id

        return pd.DataFrame(
            [[co] + [aeb] + [w]], columns=[&#34;cluster_id&#34;, &#34;avg_cluster_eb&#34;, &#34;wiener_ind&#34;]
        )

    out = sparkdf.groupby(cluster_id_colname).apply(avg_eb)

    return out


def number_of_bridges(
    sparkdf,
    src=&#34;src&#34;,
    dst=&#34;dst&#34;,
    cluster_id_colname=&#34;cluster_id&#34;,
):

    &#34;&#34;&#34;return

        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            cluster_id_colname: Graphframes-created connected components created cluster_id

     Returns:
            cluster_id: Connected components created cluster_id
            num_bridges: The number of bridges in the cluster



    example input spark dataframe

    |src|dst|cluster_id|
    +---|---|----------|
    |  f|  d|         0|
    |  f|  g|         0|
    |  b|  c|8589934592|
    |  g|  h|         0|
    |  a|  b|8589934592|
    |  h|  i|         0|
    |  h|  j|         0|
    |  d|  e|         0|
    |  e|  f|         0|


    example output spark dataframe


    |   cluster_id |   number_of_bridges |
    |-------------:|--------------------:|
    |   8589934592 |                   2 |
    |            0 |                   4 |


    &#34;&#34;&#34;
    psrc = src
    pdst = dst
    pcomponent = cluster_id_colname

    bridgesoutSchema = StructType(
        [
            StructField(&#34;cluster_id&#34;, LongType()),
            StructField(&#34;number_of_bridges&#34;, LongType()),
        ]
    )

    @pandas_udf(bridgesoutSchema, PandasUDFType.GROUPED_MAP)
    def br_p_udf(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        co = pdf[cluster_id_colname].iloc[0]  # access component id

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst)

        b = bridges(nxGraph)

        data = {&#34;cluster_id&#34;: [co], &#34;number_of_bridges&#34;: [len(list(b))]}

        return pd.DataFrame(data)

    indf = sparkdf.select(psrc, pdst, pcomponent)
    out = indf.groupby(cluster_id_colname).apply(br_p_udf)
    return out


def cluster_assortativity(
    sparkdf, src=&#34;src&#34;, dst=&#34;dst&#34;, cluster_id_colname=&#34;cluster_id&#34;
):
    &#34;&#34;&#34;calculate assortativity of a cluster

        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            cluster_id_colname: Graphframes-created connected components created cluster_id



        input spark dataframe:


    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0| 0.329|
    |  f|  g|  0.34|         0| 0.659|
    |  b|  c|  0.56|8589934592| 0.439|
    |  g|  h|  0.99|         0|0.010|
    |  a|  b|   0.4|8589934592|0.6|
    |  h|  i|   0.5|         0|0.5|
    |  h|  j|   0.8|         0| 0.199|
    |  d|  e|  0.84|         0| 0.160|
    |  e|  f|  0.65|         0|0.35|



        output spark dataframe:

    &#34;&#34;&#34;

    psrc = src
    pdst = dst

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;assortativity&#34;, FloatType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def asrt(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst)

        r = nx.degree_pearson_correlation_coefficient(nxGraph)

        # when all degrees are equal (grids or full graphs) assortativity is undefined/nan
        # However we can think this case as fully correlated so we put it as 1

        if math.isnan(r):
            r = 1.0

        co = pdf[cluster_id_colname].iloc[0]  # access component id

        return pd.DataFrame(
            [[co] + [r]],
            columns=[
                &#34;cluster_id&#34;,
                &#34;assortativity&#34;,
            ],
        )

    out = sparkdf.groupby(cluster_id_colname).apply(asrt)

    out = out.withColumn(&#34;assortativity&#34;, f.round(f.col(&#34;assortativity&#34;), 3))

    return out</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="splink_graph.cluster_metrics.cluster_assortativity"><code class="name flex">
<span>def <span class="ident">cluster_assortativity</span></span>(<span>sparkdf, src='src', dst='dst', cluster_id_colname='cluster_id')</span>
</code></dt>
<dd>
<div class="desc"><p>calculate assortativity of a cluster</p>
<pre><code>Args:
    sparkdf: imput edgelist Spark DataFrame
    src: src column name
    dst: dst column name
    cluster_id_colname: Graphframes-created connected components created cluster_id



input spark dataframe:
</code></pre>
<table>
<thead>
<tr>
<th>src</th>
<th>dst</th>
<th>weight</th>
<th>cluster_id</th>
<th>distance</th>
</tr>
</thead>
<tbody>
<tr>
<td>f</td>
<td>d</td>
<td>0.67</td>
<td>0</td>
<td>0.329</td>
</tr>
<tr>
<td>f</td>
<td>g</td>
<td>0.34</td>
<td>0</td>
<td>0.659</td>
</tr>
<tr>
<td>b</td>
<td>c</td>
<td>0.56</td>
<td>8589934592</td>
<td>0.439</td>
</tr>
<tr>
<td>g</td>
<td>h</td>
<td>0.99</td>
<td>0</td>
<td>0.010</td>
</tr>
<tr>
<td>a</td>
<td>b</td>
<td>0.4</td>
<td>8589934592</td>
<td>0.6</td>
</tr>
<tr>
<td>h</td>
<td>i</td>
<td>0.5</td>
<td>0</td>
<td>0.5</td>
</tr>
<tr>
<td>h</td>
<td>j</td>
<td>0.8</td>
<td>0</td>
<td>0.199</td>
</tr>
<tr>
<td>d</td>
<td>e</td>
<td>0.84</td>
<td>0</td>
<td>0.160</td>
</tr>
<tr>
<td>e</td>
<td>f</td>
<td>0.65</td>
<td>0</td>
<td>0.35</td>
</tr>
</tbody>
</table>
<pre><code>output spark dataframe:
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cluster_assortativity(
    sparkdf, src=&#34;src&#34;, dst=&#34;dst&#34;, cluster_id_colname=&#34;cluster_id&#34;
):
    &#34;&#34;&#34;calculate assortativity of a cluster

        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            cluster_id_colname: Graphframes-created connected components created cluster_id



        input spark dataframe:


    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0| 0.329|
    |  f|  g|  0.34|         0| 0.659|
    |  b|  c|  0.56|8589934592| 0.439|
    |  g|  h|  0.99|         0|0.010|
    |  a|  b|   0.4|8589934592|0.6|
    |  h|  i|   0.5|         0|0.5|
    |  h|  j|   0.8|         0| 0.199|
    |  d|  e|  0.84|         0| 0.160|
    |  e|  f|  0.65|         0|0.35|



        output spark dataframe:

    &#34;&#34;&#34;

    psrc = src
    pdst = dst

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;assortativity&#34;, FloatType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def asrt(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst)

        r = nx.degree_pearson_correlation_coefficient(nxGraph)

        # when all degrees are equal (grids or full graphs) assortativity is undefined/nan
        # However we can think this case as fully correlated so we put it as 1

        if math.isnan(r):
            r = 1.0

        co = pdf[cluster_id_colname].iloc[0]  # access component id

        return pd.DataFrame(
            [[co] + [r]],
            columns=[
                &#34;cluster_id&#34;,
                &#34;assortativity&#34;,
            ],
        )

    out = sparkdf.groupby(cluster_id_colname).apply(asrt)

    out = out.withColumn(&#34;assortativity&#34;, f.round(f.col(&#34;assortativity&#34;), 3))

    return out</code></pre>
</details>
</dd>
<dt id="splink_graph.cluster_metrics.cluster_avg_edge_betweenness"><code class="name flex">
<span>def <span class="ident">cluster_avg_edge_betweenness</span></span>(<span>sparkdf, src='src', dst='dst', distance_colname='distance', cluster_id_colname='cluster_id')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<pre><code>sparkdf: imput edgelist Spark DataFrame
src: src column name
dst: dst column name
distance_colname: column name where edge distance (1-weight) is available
cluster_id_colname: column that contains Graphframes-created connected components created cluster_id
</code></pre>
<p>Returns:
cluster_id: connected components created cluster_id
avg_cluster_eb: average edge betweeness for cluster_id</p>
<p>input spark dataframe:
|src|dst|weight|cluster_id|distance|
|&mdash;|&mdash;|------|----------|--------|
|
f|
d|
0.67|
0| 0.329|
|
f|
g|
0.34|
0| 0.659|
|
b|
c|
0.56|8589934592| 0.439|
|
g|
h|
0.99|
0|0.010|
|
a|
b|
0.4|8589934592|0.6|
|
h|
i|
0.5|
0|0.5|
|
h|
j|
0.8|
0| 0.199|
|
d|
e|
0.84|
0| 0.160|
|
e|
f|
0.65|
0|0.35|</p>
<pre><code>output spark dataframe:
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cluster_avg_edge_betweenness(
    sparkdf,
    src=&#34;src&#34;,
    dst=&#34;dst&#34;,
    distance_colname=&#34;distance&#34;,
    cluster_id_colname=&#34;cluster_id&#34;,
):
    &#34;&#34;&#34;
        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            distance_colname: column name where edge distance (1-weight) is available
            cluster_id_colname: column that contains Graphframes-created connected components created cluster_id

        Returns:
            cluster_id: connected components created cluster_id
            avg_cluster_eb: average edge betweeness for cluster_id

        input spark dataframe:


    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0| 0.329|
    |  f|  g|  0.34|         0| 0.659|
    |  b|  c|  0.56|8589934592| 0.439|
    |  g|  h|  0.99|         0|0.010|
    |  a|  b|   0.4|8589934592|0.6|
    |  h|  i|   0.5|         0|0.5|
    |  h|  j|   0.8|         0| 0.199|
    |  d|  e|  0.84|         0| 0.160|
    |  e|  f|  0.65|         0|0.35|



        output spark dataframe:

    &#34;&#34;&#34;

    psrc = src
    pdst = dst
    pdistance = distance_colname

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;avg_cluster_eb&#34;, FloatType()),
                StructField(&#34;wiener_ind&#34;, FloatType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def avg_eb(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst, pdistance)
        w = nx.wiener_index(nxGraph)

        edge_btwn = edge_betweenness_centrality(nxGraph, normalized=True)

        if len(edge_btwn) &gt; 0:
            aeb = round(sum(list(edge_btwn.values())) / len(edge_btwn), 3)
        else:
            aeb = 0.0

        co = pdf[cluster_id_colname].iloc[0]  # access component id

        return pd.DataFrame(
            [[co] + [aeb] + [w]], columns=[&#34;cluster_id&#34;, &#34;avg_cluster_eb&#34;, &#34;wiener_ind&#34;]
        )

    out = sparkdf.groupby(cluster_id_colname).apply(avg_eb)

    return out</code></pre>
</details>
</dd>
<dt id="splink_graph.cluster_metrics.cluster_basic_stats"><code class="name flex">
<span>def <span class="ident">cluster_basic_stats</span></span>(<span>df, src='src', dst='dst', cluster_id_colname='cluster_id', weight_colname='weight')</span>
</code></dt>
<dd>
<div class="desc"><p>show nodecount , edgecount, density and enumerate nodes per cluster/conn. components subgraph</p>
<p>example input spark dataframe:</p>
<table>
<thead>
<tr>
<th>src</th>
<th>dst</th>
<th>cluster_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>f</td>
<td>d</td>
<td>0</td>
</tr>
<tr>
<td>f</td>
<td>g</td>
<td>0</td>
</tr>
<tr>
<td>b</td>
<td>c</td>
<td>8589934592</td>
</tr>
<tr>
<td>g</td>
<td>h</td>
<td>0</td>
</tr>
<tr>
<td>a</td>
<td>b</td>
<td>8589934592</td>
</tr>
<tr>
<td>h</td>
<td>i</td>
<td>0</td>
</tr>
<tr>
<td>h</td>
<td>j</td>
<td>0</td>
</tr>
<tr>
<td>d</td>
<td>e</td>
<td>0</td>
</tr>
<tr>
<td>e</td>
<td>f</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>example output spark dataframe:</p>
<table>
<thead>
<tr>
<th>cluster_id</th>
<th>nodes</th>
<th>nodecount</th>
<th>edgecount</th>
<th>density</th>
</tr>
</thead>
<tbody>
<tr>
<td>8589934592</td>
<td>[b, a, c]</td>
<td>3</td>
<td>2</td>
<td>0.666</td>
</tr>
<tr>
<td>0</td>
<td>[h, g, f, e, d, i..]</td>
<td>7</td>
<td>7</td>
<td>0.333</td>
</tr>
</tbody>
</table></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cluster_basic_stats(
    df, src=&#34;src&#34;, dst=&#34;dst&#34;, cluster_id_colname=&#34;cluster_id&#34;, weight_colname=&#34;weight&#34;
):

    &#34;&#34;&#34;show nodecount , edgecount, density and enumerate nodes per cluster/conn. components subgraph

    example input spark dataframe:


    |src|dst|cluster_id|
    |---|---|----------|
    |  f|  d|         0|
    |  f|  g|         0|
    |  b|  c|8589934592|
    |  g|  h|         0|
    |  a|  b|8589934592|
    |  h|  i|         0|
    |  h|  j|         0|
    |  d|  e|         0|
    |  e|  f|         0|


    example output spark dataframe:

    |cluster_id|               nodes|nodecount|edgecount|density|
    |----------|--------------------|---------|---------|------|
    |8589934592|           [b, a, c]|        3|        2|0.666|
    |         0|[h, g, f, e, d, i..]|        7|        7|0.333|


    &#34;&#34;&#34;

    edgec = df.groupby(cluster_id_colname).agg(
        f.count(weight_colname).alias(&#34;edgecount&#34;)
    )
    srcdf = df.groupby(cluster_id_colname).agg(f.collect_set(src).alias(&#34;sources&#34;))
    dstdf = df.groupby(cluster_id_colname).agg(f.collect_set(dst).alias(&#34;destinations&#34;))
    allnodes = srcdf.join(dstdf, on=cluster_id_colname)
    allnodes = allnodes.withColumn(
        &#34;nodes&#34;, f.array_union(f.col(&#34;sources&#34;), f.col(&#34;destinations&#34;))
    ).withColumn(&#34;nodecount&#34;, f.size(f.col(&#34;nodes&#34;)))

    output = allnodes.join(edgec, on=cluster_id_colname)

    # density related calcs based on nodecount and max possible number of edges in an undirected graph

    output = output.withColumn(
        &#34;maxNumberOfEdgesundir&#34;, f.expr(&#34;(nodecount*(nodecount-1))/2&#34;)
    )
    output = output.withColumn(
        &#34;density&#34;, f.expr(&#34;edgecount/maxNumberOfEdgesundir&#34;)
    ).drop(&#34;sources&#34;, &#34;destinations&#34;, &#34;maxNumberOfEdgesundir&#34;)
    output = output.withColumnRenamed(cluster_id_colname, &#34;cluster_id&#34;)

    return output</code></pre>
</details>
</dd>
<dt id="splink_graph.cluster_metrics.cluster_connectivity_stats"><code class="name flex">
<span>def <span class="ident">cluster_connectivity_stats</span></span>(<span>sparkdf, src='src', dst='dst', cluster_id_colname='cluster_id')</span>
</code></dt>
<dd>
<div class="desc"><p>outputs connectivity metrics per cluster_id</p>
<pre><code>Args:
    sparkdf: imput edgelist Spark DataFrame
    src: src column name
    dst: dst column name
    cluster_id_colname: Graphframes-created connected components created cluster_id

Returns:

    node_conn: Node Connectivity measures the minimal number of vertices that can be removed to disconnect the graph.
    edge_conn: Edge connectivity measures the minimal number of edges that can be removed to disconnect the graph.
    degeneracy: a way to measure sparsity
    num_articulation_pts: how many articulation points? an articulation point is a node that if removed disconnects a graph


The larger these metrics are --&gt; the more connected the subggraph is.



input spark dataframe:
</code></pre>
<table>
<thead>
<tr>
<th>src</th>
<th>dst</th>
<th>weight</th>
<th>cluster_id</th>
<th>distance</th>
</tr>
</thead>
<tbody>
<tr>
<td>f</td>
<td>d</td>
<td>0.67</td>
<td>0</td>
<td>0.329</td>
</tr>
<tr>
<td>f</td>
<td>g</td>
<td>0.34</td>
<td>0</td>
<td>0.659</td>
</tr>
<tr>
<td>b</td>
<td>c</td>
<td>0.56</td>
<td>8589934592</td>
<td>0.439</td>
</tr>
<tr>
<td>g</td>
<td>h</td>
<td>0.99</td>
<td>0</td>
<td>0.010</td>
</tr>
<tr>
<td>a</td>
<td>b</td>
<td>0.4</td>
<td>8589934592</td>
<td>0.6</td>
</tr>
<tr>
<td>h</td>
<td>i</td>
<td>0.5</td>
<td>0</td>
<td>0.5</td>
</tr>
<tr>
<td>h</td>
<td>j</td>
<td>0.8</td>
<td>0</td>
<td>0.199</td>
</tr>
<tr>
<td>d</td>
<td>e</td>
<td>0.84</td>
<td>0</td>
<td>0.160</td>
</tr>
<tr>
<td>e</td>
<td>f</td>
<td>0.65</td>
<td>0</td>
<td>0.35</td>
</tr>
</tbody>
</table>
<pre><code>output spark dataframe:
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cluster_connectivity_stats(
    sparkdf,
    src=&#34;src&#34;,
    dst=&#34;dst&#34;,
    cluster_id_colname=&#34;cluster_id&#34;,
):
    &#34;&#34;&#34;outputs connectivity metrics per cluster_id

        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            cluster_id_colname: Graphframes-created connected components created cluster_id

        Returns:

            node_conn: Node Connectivity measures the minimal number of vertices that can be removed to disconnect the graph.
            edge_conn: Edge connectivity measures the minimal number of edges that can be removed to disconnect the graph.
            degeneracy: a way to measure sparsity
            num_articulation_pts: how many articulation points? an articulation point is a node that if removed disconnects a graph


        The larger these metrics are --&gt; the more connected the subggraph is.



        input spark dataframe:


    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0| 0.329|
    |  f|  g|  0.34|         0| 0.659|
    |  b|  c|  0.56|8589934592| 0.439|
    |  g|  h|  0.99|         0|0.010|
    |  a|  b|   0.4|8589934592|0.6|
    |  h|  i|   0.5|         0|0.5|
    |  h|  j|   0.8|         0| 0.199|
    |  d|  e|  0.84|         0| 0.160|
    |  e|  f|  0.65|         0|0.35|



        output spark dataframe:

    &#34;&#34;&#34;

    psrc = src
    pdst = dst

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;node_conn&#34;, IntegerType()),
                StructField(&#34;edge_conn&#34;, IntegerType()),
                StructField(&#34;degeneracy&#34;, IntegerType()),
                StructField(&#34;num_articulation_pts&#34;, IntegerType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def conn_eff(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst)
        nc = nx.algorithms.node_connectivity(nxGraph)
        ec = nx.algorithms.edge_connectivity(nxGraph)
        dg = max(nx.core_number(nxGraph).values())

        ap = articulation_points(nxGraph)
        num_aps = len(list(ap))

        co = pdf[cluster_id_colname].iloc[0]  # access component id

        return pd.DataFrame(
            [[co] + [nc] + [ec] + [dg] + [num_aps]],
            columns=[
                &#34;cluster_id&#34;,
                &#34;node_conn&#34;,
                &#34;edge_conn&#34;,
                &#34;degeneracy&#34;,
                &#34;num_articulation_pts&#34;,
            ],
        )

    out = sparkdf.groupby(cluster_id_colname).apply(conn_eff)

    return out</code></pre>
</details>
</dd>
<dt id="splink_graph.cluster_metrics.cluster_eb_modularity"><code class="name flex">
<span>def <span class="ident">cluster_eb_modularity</span></span>(<span>sparkdf, src='src', dst='dst', distance_colname='distance', cluster_id_colname='cluster_id')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<pre><code>sparkdf: imput edgelist Spark DataFrame
src: src column name
dst: dst column name
distance_colname: column name where edge distance (1-weight) is available
cluster_id_colname: column that contains Graphframes-created connected components created cluster_id
</code></pre>
<p>Returns:
cluster_id: connected components created cluster_id
comp_eb_modularity: modularity for cluster_id if it partitioned into 2 parts at the point where the highest edge betweenness exists</p>
<p>example input spark dataframe
|src|dst|weight|cluster_id|distance|
|&mdash;|&mdash;|------|----------|--------|
|
f|
d|
0.67|
0| 0.329|
|
f|
g|
0.34|
0| 0.659|
|
b|
c|
0.56|8589934592| 0.439|
|
g|
h|
0.99|
0|0.010|
|
a|
b|
0.4|8589934592|0.6|
|
h|
i|
0.5|
0|0.5|
|
h|
j|
0.8|
0| 0.199|
|
d|
e|
0.84|
0| 0.160|
|
e|
f|
0.65|
0|0.35|</p>
<pre><code>example output spark dataframe
</code></pre>
<table>
<thead>
<tr>
<th>cluster_id</th>
<th>comp_eb_modularity</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.400</td>
</tr>
<tr>
<td>8589934592</td>
<td>-0.04</td>
</tr>
</tbody>
</table></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cluster_eb_modularity(
    sparkdf,
    src=&#34;src&#34;,
    dst=&#34;dst&#34;,
    distance_colname=&#34;distance&#34;,
    cluster_id_colname=&#34;cluster_id&#34;,
):
    &#34;&#34;&#34;
        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            distance_colname: column name where edge distance (1-weight) is available
            cluster_id_colname: column that contains Graphframes-created connected components created cluster_id

        Returns:
            cluster_id: connected components created cluster_id
            comp_eb_modularity: modularity for cluster_id if it partitioned into 2 parts at the point where the highest edge betweenness exists


        example input spark dataframe


    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0| 0.329|
    |  f|  g|  0.34|         0| 0.659|
    |  b|  c|  0.56|8589934592| 0.439|
    |  g|  h|  0.99|         0|0.010|
    |  a|  b|   0.4|8589934592|0.6|
    |  h|  i|   0.5|         0|0.5|
    |  h|  j|   0.8|         0| 0.199|
    |  d|  e|  0.84|         0| 0.160|
    |  e|  f|  0.65|         0|0.35|


        example output spark dataframe

    |cluster_id|comp_eb_modularity|
    |----------|----------------|
    |         0| 0.400|
    |8589934592| -0.04|

    &#34;&#34;&#34;

    psrc = src
    pdst = dst
    pdistance = distance_colname

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;cluster_eb_modularity&#34;, FloatType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def cluster_eb_m(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst, pdistance)

        ## TODO: document this code
        # this is a method that calculates the modularity of a cluster if partitioned into 2 parts
        # where the split is happening where the highest edge betweeness is.

        # if modularity is negative :
        #      that means that the split just leaves singleton nodes or something like that.
        #      basically the cluster is of no interest
        # if modularity is 0 or very close to 0 :
        #      its a cluster of well connected nodes so... nothing to see here really.
        # if modularity is around 0.3+ then :
        #      its a cluster of possible interest

        def largest_edge_betweenness(G):
            centrality = edge_betweenness_centrality(
                G, weight=pdistance, normalized=True
            )
            return max(centrality, key=centrality.get)

        comp = girvan_newman(nxGraph, most_valuable_edge=largest_edge_betweenness)
        gn = tuple(sorted(c) for c in next(comp))

        co = pdf[cluster_id_colname].iloc[0]  # access component id
        nc = nx.number_of_nodes(nxGraph)

        if nc &gt; 2:
            try:
                co_eb_mod = nx_comm.modularity(nxGraph, gn)
            except ZeroDivisionError:
                raise Exception(
                    f&#34;ZeroDivisionError on component id {co}. &#34;
                    &#34;This can occur if one of the weights (distances) is zero.&#34;
                )
        else:
            co_eb_mod = -1.0

        return pd.DataFrame(
            [[co] + [co_eb_mod]],
            columns=[
                &#34;cluster_id&#34;,
                &#34;cluster_eb_modularity&#34;,
            ],
        )

    out = sparkdf.groupby(cluster_id_colname).apply(cluster_eb_m)

    return out</code></pre>
</details>
</dd>
<dt id="splink_graph.cluster_metrics.cluster_graph_hash"><code class="name flex">
<span>def <span class="ident">cluster_graph_hash</span></span>(<span>sparkdf, src='src', dst='dst', cluster_id_colname='cluster_id')</span>
</code></dt>
<dd>
<div class="desc"><p>calculate weisfeiler-lehman graph hash of a cluster</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sparkdf</code></strong></dt>
<dd>imput edgelist Spark DataFrame</dd>
<dt><strong><code>src</code></strong></dt>
<dd>src column name</dd>
<dt><strong><code>dst</code></strong></dt>
<dd>dst column name</dd>
<dt><strong><code>cluster_id_colname</code></strong></dt>
<dd>Graphframes-created connected components created cluster_id</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cluster_graph_hash(sparkdf, src=&#34;src&#34;, dst=&#34;dst&#34;, cluster_id_colname=&#34;cluster_id&#34;):
    &#34;&#34;&#34;calculate weisfeiler-lehman graph hash of a cluster


    Args:
        sparkdf: imput edgelist Spark DataFrame
        src: src column name
        dst: dst column name
        cluster_id_colname: Graphframes-created connected components created cluster_id
    &#34;&#34;&#34;
    psrc = src
    pdst = dst

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;graphhash&#34;, StringType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def gh(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst)
        h = nx.weisfeiler_lehman_graph_hash(nxGraph)
        co = pdf[cluster_id_colname].iloc[0]  # access component id

        return pd.DataFrame(
            [[co] + [h]],
            columns=[
                &#34;cluster_id&#34;,
                &#34;graphhash&#34;,
            ],
        )

    out = sparkdf.groupby(cluster_id_colname).apply(gh)
    return out</code></pre>
</details>
</dd>
<dt id="splink_graph.cluster_metrics.cluster_graph_hash_edge_attr"><code class="name flex">
<span>def <span class="ident">cluster_graph_hash_edge_attr</span></span>(<span>sparkdf, src='src', dst='dst', cluster_id_colname='cluster_id', edge_attr_col=None)</span>
</code></dt>
<dd>
<div class="desc"><p>calculate weisfeiler-lehman graph hash of a cluster taking into account the edge weights too.
weights are converted to strings for the hashing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sparkdf</code></strong></dt>
<dd>imput edgelist Spark DataFrame</dd>
<dt><strong><code>src</code></strong></dt>
<dd>src column name</dd>
<dt><strong><code>dst</code></strong></dt>
<dd>dst column name</dd>
<dt><strong><code>cluster_id_colname</code></strong></dt>
<dd>Graphframes-created connected components created cluster_id</dd>
<dt><strong><code>edge_attr_col</code></strong></dt>
<dd>edge attributes (like edge weights) column</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cluster_graph_hash_edge_attr(
    sparkdf, src=&#34;src&#34;, dst=&#34;dst&#34;, cluster_id_colname=&#34;cluster_id&#34;, edge_attr_col=None
):
    &#34;&#34;&#34;calculate weisfeiler-lehman graph hash of a cluster taking into account the edge weights too.
      weights are converted to strings for the hashing.


    Args:
        sparkdf: imput edgelist Spark DataFrame
        src: src column name
        dst: dst column name
        cluster_id_colname: Graphframes-created connected components created cluster_id
        edge_attr_col: edge attributes (like edge weights) column
    &#34;&#34;&#34;
    psrc = src
    pdst = dst

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;graphhash_ea&#34;, StringType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def gh_edge_attr(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        if edge_attr_col:
            pdf[edge_attr_col] = pdf[edge_attr_col].astype(str)

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst, [edge_attr_col])

        ghe = nx.weisfeiler_lehman_graph_hash(nxGraph, edge_attr=edge_attr_col)
        co = pdf[cluster_id_colname].iloc[0]  # access component id

        return pd.DataFrame(
            [[co] + [ghe]],
            columns=[
                &#34;cluster_id&#34;,
                &#34;graphhash_ea&#34;,
            ],
        )

    out = sparkdf.groupby(cluster_id_colname).apply(gh_edge_attr)
    return out</code></pre>
</details>
</dd>
<dt id="splink_graph.cluster_metrics.cluster_lpg_modularity"><code class="name flex">
<span>def <span class="ident">cluster_lpg_modularity</span></span>(<span>sparkdf, src='src', dst='dst', distance_colname='distance', cluster_id_colname='cluster_id')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<pre><code>sparkdf: imput edgelist Spark DataFrame
src: src column name
dst: dst column name
distance_colname: column name where edge distance (1-weight) is available
cluster_id_colname: column that contains Graphframes-created connected components created cluster_id
</code></pre>
<p>Returns:
cluster_id: connected components created cluster_id
cluster_lpg_modularity: modularity for cluster_id if it partitioned into 2 parts based on label propagation</p>
<p>example input spark dataframe
|src|dst|weight|cluster_id|distance|
|&mdash;|&mdash;|------|----------|--------|
|
f|
d|
0.67|
0| 0.329|
|
f|
g|
0.34|
0| 0.659|
|
b|
c|
0.56|8589934592| 0.439|
|
g|
h|
0.99|
0|0.010|
|
a|
b|
0.4|8589934592|0.6|
|
h|
i|
0.5|
0|0.5|
|
h|
j|
0.8|
0| 0.199|
|
d|
e|
0.84|
0| 0.160|
|
e|
f|
0.65|
0|0.35|</p>
<pre><code>example output spark dataframe
</code></pre>
<table>
<thead>
<tr>
<th>cluster_id</th>
<th>cluster_lpg_modularity</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.400</td>
</tr>
<tr>
<td>8589934592</td>
<td>-0.04</td>
</tr>
</tbody>
</table></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cluster_lpg_modularity(
    sparkdf,
    src=&#34;src&#34;,
    dst=&#34;dst&#34;,
    distance_colname=&#34;distance&#34;,
    cluster_id_colname=&#34;cluster_id&#34;,
):
    &#34;&#34;&#34;
        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            distance_colname: column name where edge distance (1-weight) is available
            cluster_id_colname: column that contains Graphframes-created connected components created cluster_id

        Returns:
            cluster_id: connected components created cluster_id
            cluster_lpg_modularity: modularity for cluster_id if it partitioned into 2 parts based on label propagation


        example input spark dataframe


    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0| 0.329|
    |  f|  g|  0.34|         0| 0.659|
    |  b|  c|  0.56|8589934592| 0.439|
    |  g|  h|  0.99|         0|0.010|
    |  a|  b|   0.4|8589934592|0.6|
    |  h|  i|   0.5|         0|0.5|
    |  h|  j|   0.8|         0| 0.199|
    |  d|  e|  0.84|         0| 0.160|
    |  e|  f|  0.65|         0|0.35|


        example output spark dataframe

    |cluster_id|cluster_lpg_modularity|
    |----------|----------------|
    |         0| 0.400|
    |8589934592| -0.04|

    &#34;&#34;&#34;

    psrc = src
    pdst = dst
    pdistance = distance_colname

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;cluster_lpg_modularity&#34;, FloatType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def cluster_lpg_m(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst, pdistance)

        ## TODO: document this code
        # this is a method that calculates the modularity of a cluster if partitioned into 2 parts
        # where the split is happening based on label propagation clustering.

        # if modularity is negative :
        #      that means that the split just leaves singleton nodes or something like that.
        #      basically the cluster is of no interest
        # if modularity is 0 or very close to 0 :
        #      its a cluster of well connected nodes so... nothing to see here really.
        # if modularity is around 0.3+ then :
        #      its a cluster of possible interest

        gn = list(nx_comm.label_propagation_communities(nxGraph))

        co = pdf[cluster_id_colname].iloc[0]  # access component id
        co_lpg_mod = nx_comm.modularity(nxGraph, gn)

        return pd.DataFrame(
            [[co] + [co_lpg_mod]],
            columns=[
                &#34;cluster_id&#34;,
                &#34;cluster_lpg_modularity&#34;,
            ],
        )

    out = sparkdf.groupby(cluster_id_colname).apply(cluster_lpg_m)

    return out</code></pre>
</details>
</dd>
<dt id="splink_graph.cluster_metrics.cluster_main_stats"><code class="name flex">
<span>def <span class="ident">cluster_main_stats</span></span>(<span>sparkdf, src='src', dst='dst', cluster_id_colname='cluster_id')</span>
</code></dt>
<dd>
<div class="desc"><p>calculate diameter / transitivity(GCC) / triangle clustering coefficient LCC / square clustering coeff</p>
<pre><code>Args:
    sparkdf: imput edgelist Spark DataFrame
    src: src column name
    dst: dst column name
    cluster_id_colname: Graphframes-created connected components created cluster_id



input spark dataframe:
</code></pre>
<table>
<thead>
<tr>
<th>src</th>
<th>dst</th>
<th>weight</th>
<th>cluster_id</th>
<th>distance</th>
</tr>
</thead>
<tbody>
<tr>
<td>f</td>
<td>d</td>
<td>0.67</td>
<td>0</td>
<td>0.329</td>
</tr>
<tr>
<td>f</td>
<td>g</td>
<td>0.34</td>
<td>0</td>
<td>0.659</td>
</tr>
<tr>
<td>b</td>
<td>c</td>
<td>0.56</td>
<td>8589934592</td>
<td>0.439</td>
</tr>
<tr>
<td>g</td>
<td>h</td>
<td>0.99</td>
<td>0</td>
<td>0.010</td>
</tr>
<tr>
<td>a</td>
<td>b</td>
<td>0.4</td>
<td>8589934592</td>
<td>0.6</td>
</tr>
<tr>
<td>h</td>
<td>i</td>
<td>0.5</td>
<td>0</td>
<td>0.5</td>
</tr>
<tr>
<td>h</td>
<td>j</td>
<td>0.8</td>
<td>0</td>
<td>0.199</td>
</tr>
<tr>
<td>d</td>
<td>e</td>
<td>0.84</td>
<td>0</td>
<td>0.160</td>
</tr>
<tr>
<td>e</td>
<td>f</td>
<td>0.65</td>
<td>0</td>
<td>0.35</td>
</tr>
</tbody>
</table>
<pre><code>output spark dataframe:
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cluster_main_stats(sparkdf, src=&#34;src&#34;, dst=&#34;dst&#34;, cluster_id_colname=&#34;cluster_id&#34;):
    &#34;&#34;&#34;calculate diameter / transitivity(GCC) / triangle clustering coefficient LCC / square clustering coeff

        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            cluster_id_colname: Graphframes-created connected components created cluster_id



        input spark dataframe:


    |src|dst|weight|cluster_id|distance|
    |---|---|------|----------|--------|
    |  f|  d|  0.67|         0| 0.329|
    |  f|  g|  0.34|         0| 0.659|
    |  b|  c|  0.56|8589934592| 0.439|
    |  g|  h|  0.99|         0|0.010|
    |  a|  b|   0.4|8589934592|0.6|
    |  h|  i|   0.5|         0|0.5|
    |  h|  j|   0.8|         0| 0.199|
    |  d|  e|  0.84|         0| 0.160|
    |  e|  f|  0.65|         0|0.35|



        output spark dataframe:




    &#34;&#34;&#34;

    psrc = src
    pdst = dst

    @pandas_udf(
        StructType(
            [
                StructField(&#34;cluster_id&#34;, LongType()),
                StructField(&#34;diameter&#34;, IntegerType()),
                StructField(&#34;transitivity&#34;, FloatType()),
                StructField(&#34;tri_clustcoeff&#34;, FloatType()),
                StructField(&#34;sq_clustcoeff&#34;, FloatType()),
            ]
        ),
        functionType=PandasUDFType.GROUPED_MAP,
    )
    def drt(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst)
        d = diameter(nxGraph)
        t = transitivity(nxGraph)
        tric = nx.average_clustering(nxGraph)
        sq = nx.square_clustering(nxGraph)
        sqc = sum(sq.values()) / len(sq.values())

        co = pdf[cluster_id_colname].iloc[0]  # access component id

        return pd.DataFrame(
            [[co] + [d] + [t] + [tric] + [sqc]],
            columns=[
                &#34;cluster_id&#34;,
                &#34;diameter&#34;,
                &#34;transitivity&#34;,
                &#34;tri_clustcoeff&#34;,
                &#34;sq_clustcoeff&#34;,
            ],
        )

    out = sparkdf.groupby(cluster_id_colname).apply(drt)

    out = (
        out.withColumn(&#34;tri_clustcoeff&#34;, f.round(f.col(&#34;tri_clustcoeff&#34;), 3))
        .withColumn(&#34;sq_clustcoeff&#34;, f.round(f.col(&#34;sq_clustcoeff&#34;), 3))
        .withColumn(&#34;transitivity&#34;, f.round(f.col(&#34;transitivity&#34;), 3))
    )
    return out</code></pre>
</details>
</dd>
<dt id="splink_graph.cluster_metrics.number_of_bridges"><code class="name flex">
<span>def <span class="ident">number_of_bridges</span></span>(<span>sparkdf, src='src', dst='dst', cluster_id_colname='cluster_id')</span>
</code></dt>
<dd>
<div class="desc"><p>return</p>
<pre><code>Args:
    sparkdf: imput edgelist Spark DataFrame
    src: src column name
    dst: dst column name
    cluster_id_colname: Graphframes-created connected components created cluster_id
</code></pre>
<p>Returns:
cluster_id: Connected components created cluster_id
num_bridges: The number of bridges in the cluster</p>
<p>example input spark dataframe</p>
<p>|src|dst|cluster_id|
+&mdash;|&mdash;|----------|
|
f|
d|
0|
|
f|
g|
0|
|
b|
c|8589934592|
|
g|
h|
0|
|
a|
b|8589934592|
|
h|
i|
0|
|
h|
j|
0|
|
d|
e|
0|
|
e|
f|
0|</p>
<p>example output spark dataframe</p>
<table>
<thead>
<tr>
<th align="right">cluster_id</th>
<th align="right">number_of_bridges</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">8589934592</td>
<td align="right">2</td>
</tr>
<tr>
<td align="right">0</td>
<td align="right">4</td>
</tr>
</tbody>
</table></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def number_of_bridges(
    sparkdf,
    src=&#34;src&#34;,
    dst=&#34;dst&#34;,
    cluster_id_colname=&#34;cluster_id&#34;,
):

    &#34;&#34;&#34;return

        Args:
            sparkdf: imput edgelist Spark DataFrame
            src: src column name
            dst: dst column name
            cluster_id_colname: Graphframes-created connected components created cluster_id

     Returns:
            cluster_id: Connected components created cluster_id
            num_bridges: The number of bridges in the cluster



    example input spark dataframe

    |src|dst|cluster_id|
    +---|---|----------|
    |  f|  d|         0|
    |  f|  g|         0|
    |  b|  c|8589934592|
    |  g|  h|         0|
    |  a|  b|8589934592|
    |  h|  i|         0|
    |  h|  j|         0|
    |  d|  e|         0|
    |  e|  f|         0|


    example output spark dataframe


    |   cluster_id |   number_of_bridges |
    |-------------:|--------------------:|
    |   8589934592 |                   2 |
    |            0 |                   4 |


    &#34;&#34;&#34;
    psrc = src
    pdst = dst
    pcomponent = cluster_id_colname

    bridgesoutSchema = StructType(
        [
            StructField(&#34;cluster_id&#34;, LongType()),
            StructField(&#34;number_of_bridges&#34;, LongType()),
        ]
    )

    @pandas_udf(bridgesoutSchema, PandasUDFType.GROUPED_MAP)
    def br_p_udf(pdf: pd.DataFrame) -&gt; pd.DataFrame:

        co = pdf[cluster_id_colname].iloc[0]  # access component id

        nxGraph = nx.Graph()
        nxGraph = nx.from_pandas_edgelist(pdf, psrc, pdst)

        b = bridges(nxGraph)

        data = {&#34;cluster_id&#34;: [co], &#34;number_of_bridges&#34;: [len(list(b))]}

        return pd.DataFrame(data)

    indf = sparkdf.select(psrc, pdst, pcomponent)
    out = indf.groupby(cluster_id_colname).apply(br_p_udf)
    return out</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="splink_graph" href="index.html">splink_graph</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="splink_graph.cluster_metrics.cluster_assortativity" href="#splink_graph.cluster_metrics.cluster_assortativity">cluster_assortativity</a></code></li>
<li><code><a title="splink_graph.cluster_metrics.cluster_avg_edge_betweenness" href="#splink_graph.cluster_metrics.cluster_avg_edge_betweenness">cluster_avg_edge_betweenness</a></code></li>
<li><code><a title="splink_graph.cluster_metrics.cluster_basic_stats" href="#splink_graph.cluster_metrics.cluster_basic_stats">cluster_basic_stats</a></code></li>
<li><code><a title="splink_graph.cluster_metrics.cluster_connectivity_stats" href="#splink_graph.cluster_metrics.cluster_connectivity_stats">cluster_connectivity_stats</a></code></li>
<li><code><a title="splink_graph.cluster_metrics.cluster_eb_modularity" href="#splink_graph.cluster_metrics.cluster_eb_modularity">cluster_eb_modularity</a></code></li>
<li><code><a title="splink_graph.cluster_metrics.cluster_graph_hash" href="#splink_graph.cluster_metrics.cluster_graph_hash">cluster_graph_hash</a></code></li>
<li><code><a title="splink_graph.cluster_metrics.cluster_graph_hash_edge_attr" href="#splink_graph.cluster_metrics.cluster_graph_hash_edge_attr">cluster_graph_hash_edge_attr</a></code></li>
<li><code><a title="splink_graph.cluster_metrics.cluster_lpg_modularity" href="#splink_graph.cluster_metrics.cluster_lpg_modularity">cluster_lpg_modularity</a></code></li>
<li><code><a title="splink_graph.cluster_metrics.cluster_main_stats" href="#splink_graph.cluster_metrics.cluster_main_stats">cluster_main_stats</a></code></li>
<li><code><a title="splink_graph.cluster_metrics.number_of_bridges" href="#splink_graph.cluster_metrics.number_of_bridges">number_of_bridges</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>